# ğŸ§  Machine Learning Lab Assignments

This repository contains my Machine Learning Lab work, implemented in **Python (Google Colab / scikit-learn)**.  
Each assignment folder includes Jupyter notebooks, outputs (plots), and documentation.  

---

## ğŸ“‚ Repository Structure
Machine-Learning-Lab/
â”‚â”€â”€ Assignment1/
â”‚ â”œâ”€â”€ ML_Assignment1.ipynb
â”‚ â”œâ”€â”€ outputs/ (generated plots if any)
â”‚ â””â”€â”€ README.md
â”‚
â”‚â”€â”€ Assignment2/
â”‚ â”œâ”€â”€ ML_Assignment2.ipynb
â”‚ â”œâ”€â”€ assignment_outputs/ (confusion matrices, ROC, learning curves, etc.)
â”‚ â”œâ”€â”€ assignment_outputs.zip
â”‚ â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md (this file)

---

## ğŸ“˜ Assignment 1: Naive Bayes & Decision Tree

### âœ¨ Objectives
- Implement and evaluate **Naive Bayes Classifier**.  
- Implement and evaluate **Decision Tree Classifier**.  
- Compare their performance using accuracy, confusion matrix, and other metrics.  

### ğŸ“Š Key Highlights
- Used UCI dataset(s) (as instructed in lab).  
- Evaluated train-test splits and reported accuracy.  
- Generated confusion matrix heatmaps.  
- Compared Decision Tree depth vs accuracy.  

ğŸ‘‰ Detailed code and outputs are in [Assignment1/](./Assignment1)  

---

## ğŸ“˜ Assignment 2: Comparative Analysis of Classifiers (SVM, MLP, RF)

### âœ¨ Objectives
- Implement and compare:
  - **Support Vector Machines (SVM)** with Linear, Polynomial, RBF, Sigmoid kernels.  
  - **Multi-Layer Perceptron (MLP)** with tuning (momentum, epoch size, learning rate).  
  - **Random Forest Classifier** with hyperparameter tuning.  

- Evaluate models on **Wine** and **Digits** datasets.  
- Apply **PCA** for dimensionality reduction and repeat experiments.  
- Report metrics: Accuracy, Precision, Recall, F1-score, AUC.  
- Generate Confusion Matrices, ROC curves, and Learning/Loss curves.  
- Compare tuned vs non-tuned performance.  

### ğŸ“Š Key Highlights
- Train-test splits: **50:50, 60:40, 70:30, 80:20**.  
- SVM (RBF) and Random Forest achieved **>90% accuracy** after tuning.  
- PCA reduced feature dimensions while keeping accuracy stable.  
- Learning curves showed overfitting risks on small data (Wine) and stable performance on Digits.  

ğŸ‘‰ Detailed code, outputs & plots are in [Assignment2/](./Assignment2)  

---

## ğŸ† Outcomes
- Hands-on experience implementing classical ML algorithms with **scikit-learn**.  
- Learned the impact of **hyperparameter tuning** and **PCA** on model performance.  
- Built a strong base for more advanced ML projects.  

---

## ğŸ“š References
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)  
- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)  

---

## ğŸ”— Author
ğŸ‘¨â€ğŸ’» **SK Naid Ahmed**  
ğŸ“Œ Roll No: 302311001005 | Section: A3  
