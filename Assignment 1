# === Colab-friendly Enhanced ML Assignment Script ===
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report

# === CONFIG ===
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
OUT_DIR = "outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# === Helpers ===
def ensure_dir(path):
    os.makedirs(path, exist_ok=True)

def plot_confusion(cm, classes, title):
    fig, ax = plt.subplots(figsize=(5, 4))
    im = ax.imshow(cm, interpolation='nearest')
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title, ylabel='True Label', xlabel='Predicted Label')
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, cm[i, j], ha="center", va="center", color="black")
    fig.tight_layout()
    plt.show()

def compute_metrics(y_true, y_pred, average='weighted'):
    acc = accuracy_score(y_true, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=average, zero_division=0)
    return acc, prec, rec, f1

# === Load Datasets ===
iris = datasets.load_iris()
bc = datasets.load_breast_cancer()

datasets_map = {
    "Iris": (iris.data, iris.target, iris.feature_names, iris.target_names),
    "BreastCancer": (bc.data, bc.target, bc.feature_names, bc.target_names),
}

splits = {}
for name, (X, y, feat_names, class_names) in datasets_map.items():
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE
    )
    splits[name] = {
        "X_train": X_train, "X_test": X_test,
        "y_train": y_train, "y_test": y_test,
        "feature_names": feat_names, "class_names": class_names
    }

# === Naive Bayes Models ===
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

def run_nb_models(dataset_name, split):
    results = []
    configs = [
        ("GaussianNB", Pipeline([('scaler', StandardScaler()), ('clf', GaussianNB())]),
         {'clf__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]}),

        ("MultinomialNB", Pipeline([('scaler', MinMaxScaler()), ('clf', MultinomialNB())]),
         {'clf__alpha': [0.1, 0.5, 1.0, 1.5], 'clf__fit_prior': [True, False]}),

        ("BernoulliNB", Pipeline([('scaler', MinMaxScaler()), ('clf', BernoulliNB())]),
         {'clf__alpha': [0.1, 0.5, 1.0], 'clf__binarize': [0.1, 0.2, 0.3, 0.4, 0.5]}),
    ]

    X_train, X_test = split["X_train"], split["X_test"]
    y_train, y_test = split["y_train"], split["y_test"]
    class_names = split["class_names"]

    for name, pipe, grid in configs:
        print(f"\n[{dataset_name}] Training {name}...")
        gs = GridSearchCV(pipe, grid, cv=cv, n_jobs=-1, scoring='accuracy', refit=True)
        gs.fit(X_train, y_train)
        y_pred = gs.predict(X_test)
        acc, prec, rec, f1 = compute_metrics(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        print(f"Best Params: {gs.best_params_}")
        print(f"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}")
        print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))
        plot_confusion(cm, class_names, f"{dataset_name} — {name} Confusion Matrix")

        results.append({
            "Dataset": dataset_name,
            "Model": name,
            "Best Params": gs.best_params_,
            "Accuracy": acc,
            "Precision": prec,
            "Recall": rec,
            "F1": f1
        })
    return pd.DataFrame(results)

# === Decision Tree Models ===
def run_dt_models(dataset_name, split):
    results = []
    X_train, X_test = split["X_train"], split["X_test"]
    y_train, y_test = split["y_train"], split["y_test"]
    feat_names = split["feature_names"]
    class_names = split["class_names"]

    for criterion in ["gini", "entropy"]:
        print(f"\n[{dataset_name}] Training DecisionTree ({criterion})...")
        grid = {
            'clf__criterion': [criterion],
            'clf__max_depth': [None, 3, 4, 5, 6, 8, 10],
            'clf__min_samples_split': [2, 4, 6, 8, 10],
            'clf__min_samples_leaf': [1, 2, 3, 4],
            'clf__class_weight': [None, 'balanced'],
            'clf__splitter': ['best', 'random']
        }
        pipe = Pipeline([('clf', DecisionTreeClassifier(random_state=RANDOM_STATE))])
        gs = GridSearchCV(pipe, grid, cv=cv, n_jobs=-1, scoring='accuracy', refit=True)
        gs.fit(X_train, y_train)
        y_pred = gs.predict(X_test)
        acc, prec, rec, f1 = compute_metrics(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        print(f"Best Params: {gs.best_params_}")
        print(f"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}")
        plot_confusion(cm, class_names, f"{dataset_name} — DT ({criterion})")

        fig, ax = plt.subplots(figsize=(12, 8))
        plot_tree(gs.best_estimator_['clf'], feature_names=feat_names, class_names=class_names,
                  filled=True, impurity=True)
        plt.title(f"{dataset_name} — Decision Tree ({criterion})")
        plt.show()

        results.append({
            "Dataset": dataset_name,
            "Model": f"DecisionTree-{criterion}",
            "Best Params": gs.best_params_,
            "Accuracy": acc,
            "Precision": prec,
            "Recall": rec,
            "F1": f1
        })
    return pd.DataFrame(results)

# === Run & Display All ===
all_results = []
for dname, split in splits.items():
    nb_df = run_nb_models(dname, split)
    dt_df = run_dt_models(dname, split)
    all_results.append(nb_df)
    all_results.append(dt_df)

final_df = pd.concat(all_results, ignore_index=True)
print("\n=== Final Model Comparison ===")
display(final_df)

# === Comparison Graphs ===
metrics = ["Accuracy", "Precision", "Recall", "F1"]
for metric in metrics:
    plt.figure(figsize=(8, 5))
    for dataset in final_df["Dataset"].unique():
        subset = final_df[final_df["Dataset"] == dataset]
        plt.plot(subset["Model"], subset[metric], marker='o', label=dataset)
    plt.title(f"{metric} Comparison")
    plt.ylabel(metric)
    plt.xticks(rotation=45)
    plt.ylim(0, 1)
    plt.legend()
    plt.tight_layout()
    plt.show()
